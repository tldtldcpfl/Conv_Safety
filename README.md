# Dialogue_Safety
- Large dialogue generation models trained to mimic human-like conversations, but still have problematic language, e.g., toxicity or bias.
- Conversation users may give helpfule feedback to the model, by what the model said is accepted or not.
- This work improves the response of the conversational models to feedback about safety failures by fine-tuning them on conv dataset. 
   
<img width="370" alt="image" src="https://github.com/tldtldcpfl/Dialogue_Safety/assets/25386336/5d1981fa-6c92-45e2-9ae7-421913390f99">
<img width="416" alt="image" src="https://github.com/tldtldcpfl/Dialogue_Safety/assets/25386336/6cb87017-a3c5-427e-b2b4-dc5b38564282">


### Resources
- https://arxiv.org/pdf/2110.07518.pdf
